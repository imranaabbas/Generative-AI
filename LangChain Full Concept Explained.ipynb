{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b292032c",
   "metadata": {},
   "source": [
    "# üìò LangChain + Google Gemini\n",
    "# AI Study Helper ‚Äì Full LangChain Concepts (Teaching Version)\n",
    "\n",
    "This notebook demonstrates ALL CORE LangChain CONCEPTS before moving to LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4519bc",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62d1fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages with latest versions\n",
    "# %pip: Jupyter magic command to run pip in the notebook kernel\n",
    "# -q: quiet mode (suppresses package download progress)\n",
    "# langchain: Core LangChain framework for building chains and agents\n",
    "# langchain-core: Core abstractions for LangChain components\n",
    "# langchain-google-genai: Integration with Google's Generative AI models\n",
    "# google-generativeai: Google's Python SDK for their generative models\n",
    "# langchain-community: Community-maintained LangChain integrations\n",
    "%pip install -q langchain langchain-core langchain-google-genai google-generativeai langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01414af4",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Set API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb53ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os module for environment variable management\n",
    "import os\n",
    "\n",
    "# Set your Google API key here - this authenticates all API calls to Google's Gemini models\n",
    "# Get your key from: https://makersuite.google.com/app/apikey\n",
    "# The key is stored in environment variable GOOGLE_API_KEY for secure access\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyADQebmyuMo_C3hjvlnf-jgm1WCMFDv-iA\"\n",
    "\n",
    "# Verify API key is set (will show *** if set)\n",
    "# This is optional - uncomment to check if API key is configured\n",
    "#if os.environ.get(\"GOOGLE_API_KEY\") and os.environ.get(\"GOOGLE_API_KEY\") != \"YOUR_GEMINI_API_KEY\":\n",
    "#    print(\"‚úÖ Google API Key is configured\")\n",
    "#else:\n",
    "#    print(\"‚ö†Ô∏è Please set your GOOGLE_API_KEY in this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558be594",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Import LangChain Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3460764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ChatGoogleGenerativeAI: LLM wrapper for Google's Generative AI models (like Gemini)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Import PromptTemplate: Framework for creating dynamic prompts with variable placeholders\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Import StrOutputParser: Converts LLM output (AIMessage objects) to plain strings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Import runnable components - building blocks for creating data pipelines:\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,    # Passes input data through unchanged (identity operation)\n",
    "    RunnableLambda,         # Wraps custom Python functions into runnable components\n",
    "    RunnableParallel        # Executes multiple runnables simultaneously and combines outputs\n",
    ")\n",
    "\n",
    "# Import StructuredTool: For creating tools with validated input/output schemas\n",
    "from langchain_core.tools import StructuredTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27614caf",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Initialize LLM (Brain of the System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fac5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ChatGoogleGenerativeAI - this is the \"brain\" of our AI system\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # Specify which Gemini model to use (fast and efficient)\n",
    "    temperature=0.3            # Temperature controls randomness: 0=deterministic, 1=very random\n",
    "                               # 0.3 = balanced (consistent but still creative)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8154ca1a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Prompt Templates (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5226ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first prompt template for explaining topics\n",
    "explain_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],  # Define the variable name that will be filled in: {topic}\n",
    "    template=\"Explain {topic} in very simple terms for a college student.\"  # Template with placeholder\n",
    ")\n",
    "\n",
    "# Create second prompt template for generating quiz questions\n",
    "quiz_prompt = PromptTemplate(\n",
    "    input_variables=[\"explanation\"],  # Define the variable: {explanation}\n",
    "    template=\"\"\"\n",
    "Based on the explanation below, create 3 simple quiz questions:\n",
    "\n",
    "{explanation}\n",
    "\"\"\"  # Multi-line template with placeholder for the explanation content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb0613",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Output Parser (Production-Safe Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1a4565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output parser to convert LLM responses to plain text strings\n",
    "# Without this, LLM returns AIMessage objects with metadata\n",
    "# StrOutputParser extracts just the text content from the response\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233d1d7",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Chains Using LCEL (| Operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "424c38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chains using LCEL (LangChain Expression Language)\n",
    "# The | operator chains components: input ‚Üí prompt ‚Üí llm ‚Üí parser ‚Üí output\n",
    "\n",
    "# First chain: explain_prompt feeds to llm, then result goes to parser\n",
    "# Flow: {topic} ‚Üí explain_prompt.format() ‚Üí llm.invoke() ‚Üí parser.invoke()\n",
    "explain_chain = explain_prompt | llm | parser\n",
    "\n",
    "# Second chain: quiz_prompt feeds to llm, then result goes to parser\n",
    "# Flow: {explanation} ‚Üí quiz_prompt.format() ‚Üí llm.invoke() ‚Üí parser.invoke()\n",
    "quiz_chain = quiz_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1829ae3",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Sequential Workflow (Explain ‚Üí Quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11761e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential workflow that chains multiple operations together\n",
    "study_helper_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}     # First: Take input as-is and pass it as 'topic' key\n",
    "    | explain_chain                      # Second: Send to explain_chain (gets explanation)\n",
    "    | quiz_chain                         # Third: Pass explanation result to quiz_chain (gets quiz)\n",
    ")\n",
    "# This creates a complete workflow: Input Topic ‚Üí Explanation ‚Üí Quiz Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee8440",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Run Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e287b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study Helper Result:\n",
      "Here are 3 simple quiz questions based on the explanation:\n",
      "\n",
      "1.  In the city analogy, what is the Operating System (OS) compared to?\n",
      "2.  According to the explanation, what is the very first piece of software that loads when you turn on your computer, without which it's just a \"fancy brick\"?\n",
      "3.  When you open multiple apps like Chrome and Spotify, the OS acts like a \"traffic cop.\" What does it do in this role?\n"
     ]
    }
   ],
   "source": [
    "# Run the sequential chain with error handling\n",
    "try:\n",
    "    # invoke() executes the chain with the given input\n",
    "    # The input flows through: topic ‚Üí explain ‚Üí quiz\n",
    "    result = study_helper_chain.invoke(\"Operating System\")\n",
    "    \n",
    "    # Print the result (this will contain the quiz questions)\n",
    "    print(\"Study Helper Result:\")\n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Catch any errors (usually from invalid API key)\n",
    "    print(f\"‚ö†Ô∏è Note: Chain execution requires a valid GOOGLE_API_KEY\")\n",
    "    print(f\"Error: {type(e).__name__}\")  # Print the type of error that occurred\n",
    "    print(f\"Chains are properly constructed. Add your API key in cell 4 to run this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafcf7b",
   "metadata": {},
   "source": [
    "## üîü Memory (Stateful Conversation)\n",
    "\n",
    "### Concepts Covered\n",
    "- Short-term memory\n",
    "- Context retention\n",
    "- Stateful AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec5b6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Memory Concept Demonstration:\n",
      "==================================================\n",
      "User: Explain Operating System\n",
      "AI: [AI Response about: Explain Operating System]\n",
      "\n",
      "User: Now explain scheduling in OS\n",
      "AI: [AI Response about: Now explain scheduling in OS]\n",
      "\n",
      "üìã Complete Conversation History:\n",
      "1. USER: Explain Operating System\n",
      "2. ASSISTANT: [AI Response about: Explain Operating System]\n",
      "3. USER: Now explain scheduling in OS\n",
      "4. ASSISTANT: [AI Response about: Now explain scheduling in OS]\n",
      "\n",
      "‚úÖ Memory demonstrates how AI retains context across multiple interactions\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate memory concept using a simple list to store conversation history\n",
    "# Modern LangChain moved memory components - using manual approach for demonstration\n",
    "\n",
    "# Create a simple conversation history storage\n",
    "conversation_history = []\n",
    "\n",
    "# Define a function to simulate stateful conversation\n",
    "def stateful_conversation(question: str, memory: list) -> str:\n",
    "    \"\"\"Simulates a conversation that remembers context\"\"\"\n",
    "    # Store question in memory\n",
    "    memory.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    # Create context from memory\n",
    "    context = \"Previous conversation:\\n\"\n",
    "    for msg in memory[-3:]:  # Keep last 3 messages for context\n",
    "        context += f\"- {msg['role']}: {msg['content']}\\n\"\n",
    "    \n",
    "    # Simulate LLM response (in real usage, this would call llm.invoke())\n",
    "    response = f\"[AI Response about: {question}]\"\n",
    "    memory.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"üß† Memory Concept Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First interaction: Explain OS\n",
    "response1 = stateful_conversation(\"Explain Operating System\", conversation_history)\n",
    "print(f\"User: Explain Operating System\")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "# Second interaction: Follow-up question (with context retained)\n",
    "response2 = stateful_conversation(\"Now explain scheduling in OS\", conversation_history)\n",
    "print(f\"User: Now explain scheduling in OS\")\n",
    "print(f\"AI: {response2}\\n\")\n",
    "\n",
    "# Show the complete conversation history\n",
    "print(\"üìã Complete Conversation History:\")\n",
    "for i, msg in enumerate(conversation_history, 1):\n",
    "    print(f\"{i}. {msg['role'].upper()}: {msg['content']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Memory demonstrates how AI retains context across multiple interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68de1f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Tools (Action Capability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5abf7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tool decorator from langchain_core for creating tools\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define a tool using the @tool decorator (modern recommended approach)\n",
    "# Tools are functions that AI agents can call to take actions\n",
    "@tool\n",
    "def syllabus_lookup(topic: str) -> str:\n",
    "    \"\"\"Fetch syllabus details for a subject\"\"\"\n",
    "    # This is a simple mock function - in reality, it could query a database\n",
    "    return f\"Syllabus for {topic}: Basics, Architecture, Scheduling, Memory Management, I/O Systems.\"\n",
    "\n",
    "# Alternative approach using Tool class directly (commented out):\n",
    "# from langchain_core.tools import Tool\n",
    "# This would be used if you need more control over tool configuration\n",
    "# syllabus_tool = Tool(\n",
    "#     name=\"SyllabusSearch\",\n",
    "#     func=syllabus_lookup,\n",
    "#     description=\"Fetch syllabus details for a subject\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a111d99",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Agents (Decision-Making AI)\n",
    "\n",
    "### üìå This is where AI becomes Agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d1fbcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGENTS: Reasoning with Tools ===\n",
      "\n",
      "Modern agent imports not available, using simple agent demo\n",
      "\n",
      "Using Simplified Agent Demonstration:\n",
      "\n",
      "[TeachingAssistant] Received query: What topics are covered in Operating System syllabus? Explain the key concepts.\n",
      "[Agent Reasoning] YES. The user is asking for topics and key concepts from an Operating System syllabus, which directly aligns with the functionality of the `syllabus_lookup` tool. This tool can retrieve the necessary ...\n",
      "\n",
      "[Agent Action] Using tool: syllabus_lookup\n",
      "[Tool Result] Retrieved syllabus data\n",
      "\n",
      "\n",
      "[Final Response]\n",
      "Based on the syllabus information provided, the Operating System (OS) syllabus covers the following core topics, each with its own set of key concepts:\n",
      "\n",
      "### What topics are covered in Operating System syllabus?\n",
      "\n",
      "The Operating System syllabus typically covers:\n",
      "1.  **Basics**\n",
      "2.  **Architecture**\n",
      "3.  **Scheduling**\n",
      "4.  **Memory Management**\n",
      "5.  **I/O Systems**\n",
      "\n",
      "### Explain the key concepts for each topic:\n",
      "\n",
      "Here's an explanation of the key concepts within each of these topics:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Basics\n",
      "\n",
      "This foundational section introduces what an Operating System is and its fundamental role.\n",
      "\n",
      "*   **Definition of an OS:** An OS is a software layer that manages computer hardware and software resources, providing common services for computer programs. It acts as an intermediary between the user and the computer hardware.\n",
      "*   **Goals of an OS:**\n",
      "    *   **Convenience:** Making the computer system easier to use.\n",
      "    *   **Efficiency:** Allowing the computer system resources to be used in an efficient manner.\n",
      "    *   **Ability to Evolve:** Permitting the development, testing, and introduction of new system functions without interfering with service.\n",
      "*   **Components of an OS:**\n",
      "    *   **Kernel:** The core of the OS, responsible for managing the system's resources and providing essential services. It's the part of the OS that is always resident in memory.\n",
      "    *   **Shell/User Interface:** The interface through which users interact with the OS (e.g., command-line interface, graphical user interface).\n",
      "    *   **System Calls:** The programmatic interface to the services provided by the OS. User programs request services from the OS via system calls.\n",
      "*   **User Mode vs. Kernel Mode:** A crucial security and protection mechanism.\n",
      "    *   **User Mode:** The mode in which user applications run. It has restricted access to system resources.\n",
      "    *   **Kernel Mode (Supervisor Mode):** The mode in which the OS kernel runs. It has full access to all hardware and memory.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Architecture\n",
      "\n",
      "This topic explores how an Operating System is structured internally.\n",
      "\n",
      "*   **Monolithic Architecture:** The entire OS (kernel, device drivers, file system, etc.) runs as a single, large program in kernel mode. It's efficient but difficult to debug and maintain.\n",
      "*   **Layered Architecture:** The OS is broken into layers, each built on top of lower layers. Each layer only interacts with the layers immediately above and below it, promoting modularity and easier debugging.\n",
      "*   **Microkernel Architecture:** A minimal kernel provides only essential services (like inter-process communication, memory management, and basic scheduling). Most other OS services (file systems, device drivers) run as user-level processes, enhancing modularity, security, and reliability.\n",
      "*   **Modular Architecture:** A modern approach where the kernel is composed of core services and dynamically loadable modules (e.g., device drivers, file systems) that can be added or removed as needed, offering flexibility without the overhead of a microkernel.\n",
      "*   **Client-Server Model:** Often associated with microkernels, where OS services are implemented as server processes, and user applications act as clients requesting services via message passing.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Scheduling\n",
      "\n",
      "Scheduling is about managing the execution of processes and threads, particularly how the CPU is allocated among them.\n",
      "\n",
      "*   **Process vs. Thread:**\n",
      "    *   **Process:** An instance of a computer program that is being executed. It has its own memory space, resources, and at least one thread.\n",
      "    *   **Thread:** A lightweight unit of execution within a process. Threads within the same process share memory and resources, making them efficient for concurrency.\n",
      "*   **CPU Scheduler:** The component of the OS that selects which process or thread should be executed by the CPU next.\n",
      "    *   **Long-Term Scheduler (Job Scheduler):** Selects processes from the job pool and loads them into memory for execution.\n",
      "    *   **Short-Term Scheduler (CPU Scheduler):** Selects from among the processes that are ready to execute and allocates the CPU to one of them.\n",
      "    *   **Medium-Term Scheduler:** Swaps processes in and out of memory to improve the mix of processes or to reduce the degree of multiprogramming.\n",
      "*   **Scheduling Criteria:** Metrics used to evaluate scheduling algorithms:\n",
      "    *   **CPU Utilization:** Keeping the CPU as busy as possible.\n",
      "    *   **Throughput:** The number of processes completed per unit time.\n",
      "    *   **Turnaround Time:** The total time from submission to completion of a process.\n",
      "    *   **Waiting Time:** The total time a process spends in the ready queue.\n",
      "    *   **Response Time:** The time from submission of a request until the first response is produced.\n",
      "*   **Scheduling Algorithms:**\n",
      "    *   **First-Come, First-Served (FCFS):** Processes are executed in the order they arrive.\n",
      "    *   **Shortest Job First (SJF):** The process with the smallest next CPU burst is executed next (can be preemptive or non-preemptive).\n",
      "    *   **Priority Scheduling:** Processes are executed based on their assigned priority (can be preemptive or non-preemptive).\n",
      "    *   **Round Robin (RR):** Each process gets a small unit of CPU time (time quantum) in a cyclic manner.\n",
      "    *   **Multilevel Queue Scheduling:** Processes are partitioned into separate queues, each with its own scheduling algorithm.\n",
      "    *   **Multilevel Feedback Queue Scheduling:** Allows processes to move between queues based on their CPU burst characteristics, preventing starvation.\n",
      "*   **Context Switching:** The process of saving the state of one process/thread and restoring the state of another so that execution can continue from the same point later. It's an overhead in multitasking.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Memory Management\n",
      "\n",
      "This topic deals with how the OS allocates and manages the computer's main memory (RAM).\n",
      "\n",
      "*   **Logical vs. Physical Address Space:**\n",
      "    *   **Logical Address:** An address generated by the CPU (also called virtual address).\n",
      "    *   **Physical Address:** An address seen by the memory unit. The Memory Management Unit (MMU) maps logical addresses to physical addresses.\n",
      "*   **Memory Allocation:**\n",
      "    *   **Contiguous Allocation:** Each process is allocated a single, contiguous block of memory.\n",
      "    *   **Non-Contiguous Allocation:** Processes can be allocated memory in non-adjacent blocks, allowing for more flexible use of memory.\n",
      "*   **Paging:** A non-contiguous memory allocation scheme where physical memory is divided into fixed-size blocks called **frames**, and logical memory is divided into blocks of the same size called **pages**. A page table maps logical pages to physical frames.\n",
      "*   **Segmentation:** A memory management scheme that supports the user's view of memory. Logical memory is divided into variable-size segments, each representing a logical unit (e.g., code, data, stack).\n",
      "*   **Virtual Memory:** A technique that allows the execution of processes that are not entirely in memory. It separates the user's logical memory from physical memory, allowing programs to be larger than physical memory.\n",
      "    *   **Demand Paging:** Pages are loaded into memory only when they are needed, reducing I/O and memory requirements.\n",
      "    *   **Swapping:** The process of moving a process (or parts of it) from main memory to secondary storage (disk) and vice versa.\n",
      "*   **Page Replacement Algorithms:** When a page fault occurs and there's no free frame, an algorithm decides which page to remove from memory:\n",
      "    *   **FIFO (First-In, First-Out):** Replaces the oldest page.\n",
      "    *   **LRU (Least Recently Used):** Replaces the page that has not been used for the longest period of time.\n",
      "    *   **Optimal Page Replacement:** Replaces the page that will not be used for the longest period of time in the future (theoretical, used for comparison).\n",
      "*   **Thrashing:** A situation where a process spends more time paging (swapping pages in and out) than executing, leading to very low CPU utilization.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. I/O Systems\n",
      "\n",
      "This topic covers how the OS manages input and output operations, interacting with various hardware devices.\n",
      "\n",
      "*   **I/O Hardware:** Understanding the components involved in I/O:\n",
      "    *   **Devices:** Keyboards, mice, printers, disks, network cards, etc.\n",
      "    *   **Device Controllers:** Electronic components that operate a peripheral device and manage data transfer between the device and the CPU/memory.\n",
      "    *   **Ports:** Connections through which devices communicate with the system.\n",
      "*   **I/O Software:**\n",
      "    *   **Device Drivers:** Software modules that provide an interface between the OS and a specific hardware device. They translate OS requests into device-specific commands.\n",
      "    *   **Interrupt Handlers:** Routines that the OS executes when an interrupt signal is received from a device, indicating that an I/O operation has completed or an event has occurred.\n",
      "*   **Polling vs. Interrupt-Driven I/O:**\n",
      "    *   **Polling:** The CPU repeatedly checks the status of an I/O device to see if it's ready for data transfer. Inefficient for slow devices.\n",
      "    *   **Interrupt-Driven I/O:** The device signals the CPU via an interrupt when it's ready, allowing the CPU to perform other tasks in the meantime.\n",
      "*   **Direct Memory Access (DMA):** A controller that allows I/O devices to transfer data directly to and from main memory without involving the CPU, significantly improving performance for high-speed devices.\n",
      "*   **Buffering:** Using a temporary memory area (buffer) to store data during I/O transfers, helping to smooth out speed differences between devices and the CPU.\n",
      "*   **Spooling:** Holding data for a device in a buffer (e.g., disk) until the device is ready. Commonly used for printers, allowing multiple print jobs to be queued.\n",
      "*   **Disk Structure:** Understanding how data is organized on a disk:\n",
      "    *   **Platters:** Circular disks that store data.\n",
      "    *   **Tracks:** Concentric circles on each platter.\n",
      "    *   **Sectors:** Arcs within a track, the smallest unit of data transfer.\n",
      "    *   **Cylinders:** The set of tracks at a given head position across all platters.\n",
      "*   **Disk Scheduling Algorithms:** Algorithms to determine the order in which disk I/O requests are serviced to minimize head movement and access time:\n",
      "    *   **FCFS (First-Come, First-Served):** Processes requests in the order they arrive.\n",
      "    *   **SSTF (Shortest Seek Time First):** Services the request closest to the current head position.\n",
      "    *   **SCAN (Elevator Algorithm):** The disk arm moves from one end of the disk to the other, servicing requests along the way, then reverses direction.\n",
      "    *   **C-SCAN (Circular SCAN):** Similar to SCAN, but only services requests in one direction, then quickly returns to the other end without servicing requests on the return trip.\n",
      "    *   **LOOK/C-LOOK:** Variations of SCAN/C-SCAN where the arm only goes as far as the furthest request in each direction, then reverses.\n",
      "\n",
      "---\n",
      "\n",
      "These topics collectively provide a comprehensive understanding of how an Operating System functions as the core software managing a computer system's resources and enabling application execution.\n",
      "\n",
      "[Agent Summary] Tool used: syllabus_lookup\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of Agentic AI - Agents that can reason and use tools\n",
    "# Modern LangChain uses create_tool_calling_agent or similar functions\n",
    "\n",
    "print(\"=== AGENTS: Reasoning with Tools ===\\n\")\n",
    "\n",
    "# Try to import modern agent creation functions\n",
    "try:\n",
    "    from langchain_core.agents import tool, AgentExecutor\n",
    "    from langchain.agents import create_tool_calling_agent\n",
    "    use_modern_agents = True\n",
    "    print(\"Using modern LangChain agent creation\")\n",
    "except ImportError:\n",
    "    use_modern_agents = False\n",
    "    print(\"Modern agent imports not available, using simple agent demo\")\n",
    "\n",
    "if use_modern_agents:\n",
    "    # Modern approach with tool-calling agent\n",
    "    try:\n",
    "        # Create a tool-calling agent (recommended approach in modern LangChain)\n",
    "        from langchain.agents import create_tool_calling_agent\n",
    "        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "        \n",
    "        # Define the prompt for the agent\n",
    "        agent_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful teaching assistant. You have access to tools that can help you answer questions.\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "        \n",
    "        # Create the tool-calling agent\n",
    "        agent = create_tool_calling_agent(\n",
    "            llm=llm,\n",
    "            tools=[syllabus_lookup],\n",
    "            prompt=agent_prompt\n",
    "        )\n",
    "        \n",
    "        # Create the executor that runs the agent loop\n",
    "        agent_executor = AgentExecutor(\n",
    "            agent=agent,\n",
    "            tools=[syllabus_lookup],\n",
    "            verbose=True,\n",
    "            handle_parsing_errors=True\n",
    "        )\n",
    "        \n",
    "        # Execute the agent\n",
    "        result = agent_executor.invoke({\n",
    "            \"input\": \"What is in the Operating System syllabus? Please use the syllabus lookup tool.\"\n",
    "        })\n",
    "        print(\"\\nAgent Response:\", result.get('output', result))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Tool-calling agent error: {e}\\n\")\n",
    "        use_modern_agents = False\n",
    "\n",
    "if not use_modern_agents:\n",
    "    # Simple Agent Simulation - demonstrates core agent concept when imports fail\n",
    "    print(\"\\nUsing Simplified Agent Demonstration:\\n\")\n",
    "    \n",
    "    class SimpleAgent:\n",
    "        \"\"\"Simple demonstration of an agent that reasons and uses tools\"\"\"\n",
    "        def __init__(self, llm, tools, name=\"TeachingAgent\"):\n",
    "            self.llm = llm\n",
    "            self.tools = {tool.name: tool for tool in tools}\n",
    "            self.name = name\n",
    "            self.memory = []\n",
    "        \n",
    "        def invoke(self, query):\n",
    "            \"\"\"Agent thinks about the problem and decides whether to use tools\"\"\"\n",
    "            print(f\"[{self.name}] Received query: {query['input']}\")\n",
    "            \n",
    "            # Step 1: Reason about the task\n",
    "            reasoning_prompt = f\"\"\"You are {self.name}. Analyze this request:\n",
    "            \n",
    "Request: {query['input']}\n",
    "\n",
    "Available tools: {list(self.tools.keys())}\n",
    "\n",
    "Should you use a tool for this? Respond briefly with YES or NO, then explain your reasoning in 1-2 sentences.\"\"\"\n",
    "            \n",
    "            reasoning_response = llm.invoke(reasoning_prompt)\n",
    "            reasoning_text = reasoning_response.content if hasattr(reasoning_response, 'content') else str(reasoning_response)\n",
    "            print(f\"[Agent Reasoning] {reasoning_text[:200]}...\\n\")\n",
    "            \n",
    "            # Step 2: Decide whether to use a tool\n",
    "            if \"YES\" in reasoning_text.upper():\n",
    "                # Use the available tool\n",
    "                tool_name = list(self.tools.keys())[0]  # Use first available tool\n",
    "                print(f\"[Agent Action] Using tool: {tool_name}\")\n",
    "                \n",
    "                try:\n",
    "                    tool_result = self.tools[tool_name].invoke(query['input'])\n",
    "                    print(f\"[Tool Result] Retrieved syllabus data\\n\")\n",
    "                    \n",
    "                    # Step 3: Synthesize response with tool output\n",
    "                    synthesis_prompt = f\"\"\"Based on this syllabus information:\n",
    "{tool_result}\n",
    "\n",
    "Answer this question: {query['input']}\n",
    "\n",
    "Provide a helpful, educational response.\"\"\"\n",
    "                    \n",
    "                    final_response = llm.invoke(synthesis_prompt)\n",
    "                    final_text = final_response.content if hasattr(final_response, 'content') else str(final_response)\n",
    "                    \n",
    "                    return {\n",
    "                        \"output\": final_text,\n",
    "                        \"tool_used\": tool_name,\n",
    "                        \"reasoning\": reasoning_text\n",
    "                    }\n",
    "                except Exception as tool_error:\n",
    "                    print(f\"[Tool Error] {tool_error}\")\n",
    "                    return {\"output\": \"Could not use tool\", \"error\": str(tool_error)}\n",
    "            else:\n",
    "                # Answer directly without tools\n",
    "                direct_prompt = f\"Question: {query['input']}\\n\\nAnswer based on your knowledge:\"\n",
    "                direct_response = llm.invoke(direct_prompt)\n",
    "                direct_text = direct_response.content if hasattr(direct_response, 'content') else str(direct_response)\n",
    "                \n",
    "                return {\n",
    "                    \"output\": direct_text,\n",
    "                    \"tool_used\": None,\n",
    "                    \"reasoning\": reasoning_text\n",
    "                }\n",
    "    \n",
    "    # Create and run the simple agent\n",
    "    simple_agent = SimpleAgent(llm, [syllabus_lookup], \"TeachingAssistant\")\n",
    "    \n",
    "    try:\n",
    "        result = simple_agent.invoke({\n",
    "            \"input\": \"What topics are covered in Operating System syllabus? Explain the key concepts.\"\n",
    "        })\n",
    "        print(f\"\\n[Final Response]\\n{result['output']}\")\n",
    "        print(f\"\\n[Agent Summary] Tool used: {result.get('tool_used', 'None')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Agent execution error: {e}\")\n",
    "        print(\"Fallback: Calling tool directly...\")\n",
    "        # Ultimate fallback: just use the tool and LLM without agent logic\n",
    "        try:\n",
    "            tool_output = syllabus_lookup.invoke(\"Operating System\")\n",
    "            print(f\"Syllabus retrieved: {tool_output[:200]}...\")\n",
    "        except Exception as tool_error:\n",
    "            print(f\"Tool error: {tool_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c405727",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ RunnableLambda (Custom Logic Inside Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8afc2dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Result:\n",
      "Okay, imagine your computer is a bustling university campus.\n",
      "\n",
      "The **Operating System (OS)** is like the **University Administrator** or the **Dean of Students**.\n",
      "\n",
      "Here's what it does:\n",
      "\n",
      "1.  **Manages Resources (The Campus Facilities):**\n",
      "    *   **CPU (Main Processor):** This is like the main lecture \n"
     ]
    }
   ],
   "source": [
    "# Define a custom Python function that will be used in the chain\n",
    "# This function takes LLM output and performs custom processing\n",
    "def summarize(text: str) -> str:\n",
    "    \"\"\"Extract first 300 characters from text\"\"\"\n",
    "    # Use Python slicing to get the first 300 characters\n",
    "    # If text is empty, return a default message\n",
    "    return text[:300] if text else \"No text to summarize\"\n",
    "\n",
    "# Create a RunnableLambda wrapper around the custom function\n",
    "# RunnableLambda converts regular Python functions into runnable components\n",
    "summary_step = RunnableLambda(summarize)\n",
    "\n",
    "# Compose the chain: explain_chain output ‚Üí summarize\n",
    "# The | operator pipes the explain chain result into the summarization function\n",
    "summary_chain = explain_chain | summary_step\n",
    "\n",
    "# Run the summary chain\n",
    "try:\n",
    "    # invoke() passes the input through: topic ‚Üí explain ‚Üí summarize\n",
    "    result = summary_chain.invoke({\"topic\": \"Operating System\"})\n",
    "    print(\"Summary Result:\")\n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Error handling for API or execution issues\n",
    "    print(f\"‚ö†Ô∏è Note: Requires valid GOOGLE_API_KEY\")\n",
    "    print(f\"Error: {type(e).__name__}\")\n",
    "    print(f\"This chain successfully combines LLM output with custom Python functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9abff8",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Parallel Chains (Multiple Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6dd3bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Chain Results:\n",
      "Explanation: Okay, imagine your computer is like a bustling city.\n",
      "\n",
      "The **Operating System (OS)** is like the **city manager or the government** of that city.\n",
      "\n",
      "Here's what that means:\n",
      "\n",
      "1.  **It Manages Everything (The Boss):**\n",
      "    *   Just like a city manager makes sure the roads are paved, electricity runs, and services are available, the OS manages all the parts of your computer: the **CPU** (the brain), the **RAM** (temporary workspace), the **hard drive** (long-term storage), your **keyboard, mouse, screen, printer**, etc.\n",
      "    *   It decides *who* gets to use *what* and *when*.\n",
      "\n",
      "2.  **It Runs Your Programs (The Business District):**\n",
      "    *   When you open an app (like Chrome, Word, or a game), that's like a new business opening in the city. The OS is responsible for giving that app the resources it needs (CPU time, RAM space) to run smoothly.\n",
      "    *   It also makes sure different apps don't crash into each other or hog all the resources.\n",
      "\n",
      "3.  **It Provides a User Interface (The City Map & Information Desk):**\n",
      "    *   This is how you interact with the computer. It's the desktop, the icons, the menus, the windows you click on.\n",
      "    *   Without an OS, you'd have to type complex codes just to open a file. The OS makes it easy and intuitive for you to tell the computer what you want to do.\n",
      "\n",
      "4.  **It Organizes Your Files (The Records Office/Library):**\n",
      "    *   When you save a document or download a picture, the OS is responsible for storing it on your hard drive, remembering where it is, and letting you find it later (in folders, etc.).\n",
      "\n",
      "5.  **It Handles Devices (The Utilities Department):**\n",
      "    *   When you plug in a new printer, USB drive, or connect your webcam, the OS is what recognizes it, installs the necessary drivers, and makes sure it works with your computer and other programs.\n",
      "\n",
      "**In very simple terms:**\n",
      "\n",
      "The Operating System is the **master program** that makes your computer usable. It's the **bridge between you (the user) and the raw hardware** of the computer. Without it, your computer would just be a pile of expensive, inert parts that don't know how to talk to each other or to you.\n",
      "\n",
      "**Think of it as:**\n",
      "\n",
      "*   **The conductor of an orchestra:** Making sure all the different instruments (hardware) play together in harmony to create music (a working computer).\n",
      "*   **The brain of the computer:** It's always running in the background, managing everything so you can get your work done or play your games.\n",
      "\n",
      "**Examples:** Windows, macOS, Linux, Android, iOS are all different types of Operating Systems.\n",
      "\n",
      "Quiz: Here are 3 simple quiz questions based on the explanation:\n",
      "\n",
      "---\n",
      "\n",
      "**Quiz Questions:**\n",
      "\n",
      "1.  In the super simple explanation, what is the Operating System (OS) compared to in a restaurant?\n",
      "    *   **Hint:** Think about who runs the show!\n",
      "\n",
      "2.  What is one main job of the Operating System (OS) when it comes to your apps (like Chrome or Word)?\n",
      "    *   **Hint:** What does it do when you open and close them?\n",
      "\n",
      "3.  Can you name one common example of an Operating System (OS) that you might find on a computer or smartphone?\n",
      "    *   **Hint:** Think of the names of software that powers your device.\n"
     ]
    }
   ],
   "source": [
    "# Create a parallel chain that executes multiple chains simultaneously\n",
    "# Instead of sequential (A ‚Üí B ‚Üí C), parallel chains run A and B at the same time\n",
    "parallel_chain = RunnableParallel(\n",
    "    explanation=explain_chain,      # Run explain_chain and store result as 'explanation' key\n",
    "    quiz=study_helper_chain         # Run study_helper_chain and store result as 'quiz' key\n",
    ")\n",
    "\n",
    "# Run parallel chains\n",
    "try:\n",
    "    # invoke() runs both chains concurrently on the same input\n",
    "    result = parallel_chain.invoke({\"topic\": \"Operating System\"})\n",
    "    \n",
    "    # Print results from both parallel executions\n",
    "    print(\"Parallel Chain Results:\")\n",
    "    print(\"Explanation:\", result.get(\"explanation\", \"\"))  # Get explanation result\n",
    "    print(\"\\nQuiz:\", result.get(\"quiz\", \"\"))               # Get quiz result\n",
    "    \n",
    "except Exception as e:\n",
    "    # Error handling\n",
    "    print(f\"‚ö†Ô∏è Note: Requires valid GOOGLE_API_KEY\")\n",
    "    print(f\"Error: {type(e).__name__}\")\n",
    "    print(f\"Parallel chains execute multiple outputs simultaneously for efficiency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc254d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Callbacks (Tracing & Observability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1190677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Callbacks & Observability Demonstration:\n",
      "==================================================\n",
      "\n",
      "üìç Simulating chain execution with tracing:\n",
      "\n",
      "[TraceObserver] Chain Starting\n",
      "[TraceObserver] LLM Starting - Input: {'prompt': 'Explain Operating System in simple terms'}\n",
      "  [LLM Processing...]\n",
      "\n",
      "[TraceObserver] LLM Completed\n",
      "[TraceObserver] Chain Completed - Output received\n",
      "\n",
      "üìã Trace Log (All Observed Events):\n",
      "--------------------------------------------------\n",
      "1. [TraceObserver] Chain Starting\n",
      "2. [TraceObserver] LLM Starting - Input: {'prompt': 'Explain Operating System in simple terms'}\n",
      "3. [TraceObserver] LLM Completed\n",
      "4. [TraceObserver] Chain Completed - Output received\n",
      "\n",
      "‚úÖ Callbacks demonstrate how to trace and observe AI execution\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate callbacks/observability concept using a custom logging handler\n",
    "# Modern LangChain callbacks require advanced setup - using simple demonstration\n",
    "\n",
    "class SimpleCallbackHandler:\n",
    "    \"\"\"Simple callback handler to demonstrate tracing and observability\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"Observer\"):\n",
    "        self.name = name\n",
    "        self.events = []\n",
    "    \n",
    "    def on_llm_start(self, serialized, inputs, **kwargs):\n",
    "        \"\"\"Called when LLM execution starts\"\"\"\n",
    "        event = f\"[{self.name}] LLM Starting - Input: {inputs}\"\n",
    "        print(event)\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        \"\"\"Called when LLM execution completes\"\"\"\n",
    "        event = f\"[{self.name}] LLM Completed\"\n",
    "        print(event)\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "        \"\"\"Called when chain execution starts\"\"\"\n",
    "        event = f\"[{self.name}] Chain Starting\"\n",
    "        print(event)\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        \"\"\"Called when chain execution completes\"\"\"\n",
    "        event = f\"[{self.name}] Chain Completed - Output received\"\n",
    "        print(event)\n",
    "        self.events.append(event)\n",
    "\n",
    "print(\"üìä Callbacks & Observability Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a callback handler instance\n",
    "handler = SimpleCallbackHandler(name=\"TraceObserver\")\n",
    "\n",
    "# Simulate a chain execution with callbacks\n",
    "print(\"\\nüìç Simulating chain execution with tracing:\\n\")\n",
    "handler.on_chain_start({}, {\"input\": \"Explain Operating System\"})\n",
    "handler.on_llm_start({}, {\"prompt\": \"Explain Operating System in simple terms\"})\n",
    "print(\"  [LLM Processing...]\\n\")\n",
    "handler.on_llm_end({\"response\": \"An Operating System manages computer resources...\"})\n",
    "handler.on_chain_end({\"output\": \"Complete response\"})\n",
    "\n",
    "print(\"\\nüìã Trace Log (All Observed Events):\")\n",
    "print(\"-\" * 50)\n",
    "for i, event in enumerate(handler.events, 1):\n",
    "    print(f\"{i}. {event}\")\n",
    "\n",
    "print(\"\\n‚úÖ Callbacks demonstrate how to trace and observe AI execution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
