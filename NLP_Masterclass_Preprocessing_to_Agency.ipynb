{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "49ead5c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49ead5c5",
        "outputId": "b81a4c49-0763-40a0-88e2-2043ffca491c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ SETUP COMPLETE: All libraries and dictionaries loaded.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# MASTER SCRIPT: THE END-TO-END NLP JOURNEY\n",
        "# =================================================================\n",
        "\n",
        "# --- 1. SETUP & LIBRARIES ---\n",
        "# We install sentence-transformers for the 'Semantic Brain' in Module 3\n",
        "!pip install -U sentence-transformers -q\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Download the 'Rulebooks' (dictionaries) for language processing\n",
        "nltk.download('punkt')      # For splitting sentences into words\n",
        "nltk.download('stopwords')  # For identifying filler words (is, the, etc.)\n",
        "nltk.download('wordnet')    # For finding the dictionary roots of words\n",
        "nltk.download('omw-1.4')    # Supporting data for lemmatization\n",
        "nltk.download('punkt_tab')  # Required for word tokenization based on the error\n",
        "\n",
        "print(\"üöÄ SETUP COMPLETE: All libraries and dictionaries loaded.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6c79faf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c79faf3",
        "outputId": "abf65f98-8820-4856-9a7d-aac55a6aa5c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è --- MODULE 1: CLEANING & PREPROCESSING ---\n",
            "1. RAW TEXT: The system is SEARCHING for 'AI Conferences' üìÖ in NYC... and it's EXCITING! üöÄ\n",
            "2. TOKENIZED (Split & Lowercase): ['the', 'system', 'is', 'searching', 'for', \"'ai\", 'conferences', \"'\", 'üìÖ', 'in', 'nyc', '...', 'and', 'it', \"'s\", 'exciting', '!', 'üöÄ']\n",
            "3. CLEANED (No Noise/Filler): ['system', 'searching', 'conferences', 'nyc', 'exciting']\n",
            "4. LEMMATIZED (Root Words): ['system', 'searching', 'conference', 'nyc', 'exciting']\n",
            "üí° CONCEPT: We turned 'messy speech' into 'clean keywords' for the AI.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# MODULE 1: THE CLEANING PIPELINE (Foundation)\n",
        "# =================================================================\n",
        "print(\"üõ†Ô∏è --- MODULE 1: CLEANING & PREPROCESSING ---\")\n",
        "\n",
        "# Our raw input: messy, mixed case, symbols, and filler words\n",
        "raw_input = \"The system is SEARCHING for 'AI Conferences' üìÖ in NYC... and it's EXCITING! üöÄ\"\n",
        "print(f\"1. RAW TEXT: {raw_input}\")\n",
        "\n",
        "# Step 1: Lowercasing & Tokenization\n",
        "# Converts to lowercase and splits into a list of words\n",
        "tokens = word_tokenize(raw_input.lower())\n",
        "print(f\"2. TOKENIZED (Split & Lowercase): {tokens}\")\n",
        "\n",
        "# Step 2: Noise Removal (Stopwords & Symbols)\n",
        "# Removes emojis, punctuation, and common filler words like 'is' or 'the'\n",
        "stop_words = set(stopwords.words('english'))\n",
        "clean_tokens = [w for w in tokens if w.isalnum() and w not in stop_words]\n",
        "print(f\"3. CLEANED (No Noise/Filler): {clean_tokens}\")\n",
        "\n",
        "# Step 3: Lemmatization\n",
        "# Reduces words to their dictionary root (e.g., 'searching' -> 'search')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "processed_tokens = [lemmatizer.lemmatize(w) for w in clean_tokens]\n",
        "clean_sentence = \" \".join(processed_tokens)\n",
        "print(f\"4. LEMMATIZED (Root Words): {processed_tokens}\")\n",
        "print(f\"üí° CONCEPT: We turned 'messy speech' into 'clean keywords' for the AI.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eac65ce5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eac65ce5",
        "outputId": "54204d11-d30d-4f99-e162-72461201beda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä --- MODULE 2: VECTORIZATION (TF-IDF) ---\n",
            "5. TF-IDF MATRIX (The 'Math Map' of our words):\n",
            "              artificial  conference  data  exciting  intelligence  learning  \\\n",
            "Our Sentence        0.00        0.45  0.00      0.45          0.00      0.00   \n",
            "AI Lesson           0.47        0.00  0.00      0.00          0.47      0.36   \n",
            "ML Training         0.00        0.00  0.42      0.00          0.00      0.32   \n",
            "\n",
            "              machine  models  need   nyc  searching  students  system  \\\n",
            "Our Sentence     0.00    0.00  0.00  0.45       0.45      0.00    0.45   \n",
            "AI Lesson        0.00    0.00  0.00  0.00       0.00      0.47    0.00   \n",
            "ML Training      0.42    0.42  0.42  0.00       0.00      0.00    0.00   \n",
            "\n",
            "              technology  training  \n",
            "Our Sentence        0.00      0.00  \n",
            "AI Lesson           0.47      0.00  \n",
            "ML Training         0.00      0.42  \n",
            "üí° CONCEPT: Every word is now a number. Machines can now 'calculate' language.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# MODULE 2: VECTORIZATION (The Mathematical Bridge)\n",
        "# =================================================================\n",
        "print(\"üìä --- MODULE 2: VECTORIZATION (TF-IDF) ---\")\n",
        "\n",
        "# Let's compare our cleaned sentence to other known sentences\n",
        "knowledge_base = [\n",
        "    clean_sentence,\n",
        "    \"students learning artificial intelligence technology\",\n",
        "    \"machine learning models need data training\"\n",
        "]\n",
        "\n",
        "# Create the TF-IDF Vectorizer\n",
        "# This calculates word importance (Rare words = High score)\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(knowledge_base)\n",
        "\n",
        "# Convert to a Table for visualization\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),\n",
        "                        columns=vectorizer.get_feature_names_out(),\n",
        "                        index=['Our Sentence', 'AI Lesson', 'ML Training'])\n",
        "\n",
        "print(\"5. TF-IDF MATRIX (The 'Math Map' of our words):\")\n",
        "print(df_tfidf.round(2))\n",
        "print(\"üí° CONCEPT: Every word is now a number. Machines can now 'calculate' language.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9baef51a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9baef51a",
        "outputId": "58bde720-a52b-415a-88d8-43a513cf9520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† --- MODULE 3: SEMANTIC SEARCH & AGENTIC ACTION ---\n",
            "6. USER INTENT DETECTED: system searching conference nyc exciting\n",
            "7. BEST MATCHING ACTION: Action: Book Travel (flight, NYC, tickets, conference)\n",
            "8. CONFIDENCE SCORE: 0.4623\n",
            "üöÄ AGENT DECISION: Match Confirmed. Executing  Book Travel (flight, NYC, tickets, conference)...\n",
            "\n",
            "============================================================\n",
            "‚úÖ NLP MASTERCLASS COMPLETE!\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# MODULE 3: SEMANTIC UNDERSTANDING & AGENCY (The Brain)\n",
        "# =================================================================\n",
        "print(\"üß† --- MODULE 3: SEMANTIC SEARCH & AGENTIC ACTION ---\")\n",
        "\n",
        "# Load a Pre-trained Transformer Model (A 'Brain' that understands meaning)\n",
        "# This model represents every sentence as a 384-dimensional 'Meaning Vector'\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Define the Agent's potential Actions (Tools)\n",
        "agent_actions = [\n",
        "    \"Action: Book Travel (flight, NYC, tickets, conference)\",\n",
        "    \"Action: Data Science (models, training, cleanup)\",\n",
        "    \"Action: Technical Support (error, password, system)\"\n",
        "]\n",
        "\n",
        "# Generate 'Embeddings' (Deep-meaning math) for the tools and our query\n",
        "tool_embeddings = model.encode(agent_actions, convert_to_tensor=True)\n",
        "query_embedding = model.encode(clean_sentence, convert_to_tensor=True)\n",
        "\n",
        "# SEMANTIC SEARCH: Match the query to the best tool based on CONCEPT\n",
        "# Note: It matches 'NYC' and 'Search' to 'Book Travel' even without identical words\n",
        "search_results = util.semantic_search(query_embedding, tool_embeddings, top_k=1)[0]\n",
        "best_match = agent_actions[search_results[0]['corpus_id']]\n",
        "confidence = search_results[0]['score']\n",
        "\n",
        "print(f\"6. USER INTENT DETECTED: {clean_sentence}\")\n",
        "print(f\"7. BEST MATCHING ACTION: {best_match}\")\n",
        "print(f\"8. CONFIDENCE SCORE: {confidence:.4f}\")\n",
        "\n",
        "# AGENTIC LOGIC: Decision Gate\n",
        "# If the AI is confident, it acts. If not, it asks for help.\n",
        "THRESHOLD = 0.45\n",
        "if confidence > THRESHOLD:\n",
        "    print(f\"üöÄ AGENT DECISION: Match Confirmed. Executing {best_match.split(':')[1]}...\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è AGENT DECISION: Confidence too low. Asking user for clarification.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n‚úÖ NLP MASTERCLASS COMPLETE!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}